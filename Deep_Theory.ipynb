{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e2c2bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (4083610854.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    1:Sigmoid Activation function:\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "1:Sigmoid Activation function:\n",
    "       Range[0,1]\n",
    "       reduces the outliers in the data\n",
    "       f(x) = 1/1+pow(e)-x \n",
    "        \n",
    "2:tanh Activation function:\n",
    "       Range[-1,1]\n",
    "\n",
    "3:ReLU Activation function:\n",
    "       The gradient would be either 0 or constant \n",
    "       f(x) = max(0,x) \n",
    "       It solves the vanishing gradient problem i,e when gradient becomes very small , it vanishes  \n",
    "       So what relu does while updating the weights using gradient(derivative) , if the gradient becomes v small , it stops\n",
    "        \n",
    "4:Softmax Activation function:\n",
    "       it is the generalization of the logistic function\n",
    "       it is used in various multiclass classification problems.\n",
    "       eg : when we classify digits\n",
    "        \n",
    "        \n",
    "OPTIMIZATION PROBLEM: \n",
    "We find the optimal weights and bias values that will minimize the loss function (we use gradient descent algo for the optimization)\n",
    "        \n",
    "    \n",
    "LOSS functions:\n",
    "    1: RMSE \n",
    "    2: Negative log likelihood (equivalent to cross-entropy) -> it is usually used when dealing with the classification problem\n",
    "    2: it is negative , so minimizing it is equivalent to maximizing the prob . M : no of classes (outputs) , N : no of samples in the data set\n",
    "    \n",
    "    \n",
    "Gradient Descent : We calculate the overall loss across all of the training dataset and then we calculate the gradeint\n",
    "Stochastic Gradient Descent: We compute the gradient and parameter vector update after every single training sample .\n",
    "Mini Batch Gradient Descent : If we use the subset of the original training dataset     \n",
    "    \n",
    "    \n",
    "    \n",
    "We use normalization(min-max normalization or z score normalization) or (meta-heuristic) to make sure gradient descent will converge faster and more accurately\n",
    "\n",
    "                \n",
    "HYPER PARAMETERS:\n",
    "\n",
    "1: Learning Rate\n",
    "\n",
    "2: Momentum:\n",
    "        There are local minimas and global minimas . (PROBLEM) , so to solve this we can use genetic algos or simulated annealing\n",
    "momentum helps to escape from the local minimum. helps to unstuck. momentum [0,1]\n",
    "\n",
    "3:Regularization\n",
    "    use to contol overfitting. DROPOUT is an inexpensive regularization method. Means we can get rid of some neurons. Works well with stochastic GD . we apply in the hidden layer . it is not a good solution when no of training is set is millions  \n",
    "                \n",
    "        \n",
    "Metrics:\n",
    "    \n",
    "1: binary_accuracy -> binary classification problems , sigmoid\n",
    "2: categorical_accuracy -> multiclass classification problem    \n",
    "3: accuracy -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a607e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
